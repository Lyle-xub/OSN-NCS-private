# OSN-SONN

Diffractive optical neural networks (DONNs), despite their theoretical ability to achieve high parallelism and high bandwidth, are significantly limited by the dense and float information generated by frame-based sensors at low operating frequency. To address this issue, we propose innovative optical spiking neurons (OSNs) that mimic biological neuronal behavior to generate sparse and binary information, which enables more efficient information transmission and thus dramatically improves the speed and energy efficiency of optical neural networks. Based on OSNs, we have built a system capable of processing visual information at unprecedented speeds of up to 3,649 frames per second—over 30 times faster than the previous reported results from DONN with similar platform but frame-based processing.

Here, we provide the data and code associated with the paper titled "Optical Spiking Neurons Enable High-Speed and Energy-Efficient Optical Neural Networks."


## Notes
### Metavision
Some of the codes use the raw data reader provided by Metavision. Therefore, they require installing the Metavision Essentials package available here: [ https://www.prophesee.ai/metavision-intelligence-essentials-download/ ] We used Metavision Essentials 2.3.0 or 4.3.0. Note that these require specific versions of Python (3.8 or 3.9 for Metavision 4.3.0), and that the codes were tested only with Python 3.9.18. Other versions of Python will likely not work. It is important to follow closely the installation guidelines [ https://docs.prophesee.ai/stable/installation/windows.html ] until the "Installation" section included and to run the executable file with admin rights. 
### ALP
https://github.com/wavefrontshaping/ALP4lib

### Solid-state fibre laser
https://github.com/complight/FisbaReadyBeam   
If you are using Windows, please refer to the modified version of the code provided by us.


### Drivers
Here, we provide the serial numbers of our hardware, including the DMD, SLM, laser, and event sensor, for reference.

Solid-state fibre laser (READYBeam™, FISBA AG) 

DMD (V-9001c VIS, ViALUX GmbH).

Phase SLM (X15223-16, HAMAMATSU) 

Neuromorphic vision sensor (PEK4I36HDCD CS, PROPHESE E S.A.).

To use the code, please contact the manufacturer to provide the necessary drivers and libraries.

 
## Data Preprocessing
We provide an easy access to the used datasets in the [[google drive]](https://drive.google.com/drive/folders/1c71Ei1OQxy5ee4EXCoVPkqI6_U5q4FNF?usp=sharing)

**Data_preproccess.ipynb**: A Jupyter Notebook containing various data preprocessing steps.
**Weizmann Dataset**: The raw data is stored in `.mat` files as matrices, which need to be converted to `.avi` videos. These videos are then converted into event data using the official SDK for event cameras, and finally, the event data is converted into event frames.
**KTH Dataset**: The raw `.raw` files, captured by the event camera, are directly converted into event frames.
Subsequent processing includes resizing `.png` images and generating `.hdf5` and `.npy` files for training.

1. **Weizmann Dataset Preprocessing**:
   - Convert `.mat` files to `.avi` video format.
   - Use the official event camera SDK to convert the video into event data.
   - Convert the event data into event frames.

2. **KTH Dataset Preprocessing**:
   - Merge all videos into one total video.
   - Convert the combined video to raw event data using an event camera.
   - Convert `.raw` files to event frames.

3. **General Processing**:
   - Resize `.png` images.
   - Save the data as `.hdf5` and `.npy` files for model training.


## Guidelines

  
### BAT
This folder contains the code implementation for the BAT. 
To start the training process for the BAT, use the command below:
``` 
$ cd BAT
$ python train_donn.py
```
This command will initiate the training script for the BAT using the specified configurations in config.yaml
If you face GPU memory issues during training, reduce batch size or change model size.

### Function Details

The system folder contains the code using for simulation and physical online training.  The file expagent.py serves as the main entry point for launching the devices. It can perform various tasks, such as: model training, device control, and image processing. The file agent.py provide simulation function. The config and parameters can bet set by the parameters.py. Other files such as exputils and utils.py provide auxiliary functions for the simulation and physical training.

### S3NN and S4NN Training

This project contains code for training the KTH and Weizmann datasets using the S3NN and S4NN architectures, as well as related data preprocessing and model evaluation processes.

#### File Descriptions

- **KTH_S3NN.py**: Code for training the KTH dataset using the S3NN architecture.
- **Wei_S3NN.py**: Code for training the Weizmann dataset using the S3NN architecture.
- **KTH_S4NN_0.py**: Code for training the KTH dataset using the S4NN architecture. This script will generate seven models, with distinct workflows for the first three classes and the last three classes:
  - For the first three classes: Modify the file paths in the configuration file four times, run the script four times to generate four models (including a model to distinguish between the first three and the last three classes).
  - For the last three classes: First, modify the `forward` function in the `donn` model, then change the file paths in the configuration file three times, run the script three times to generate three models.
- **KTH_S4NN_1.py**: Loads the seven pre-trained models to obtain predictions from each model. During use, switch `path_1~4`, and the results will be saved in `.npy` files.
- **KTH_S4NN_2.py**: Computes the final accuracy based on the `.npy` files generated by `KTH_S4NN_1.py`.
- **Calculating Video Accuracy**: When calculating video accuracy, all scripts load an `.npy` lookup table. This file contains details about which video each event frame belongs to, along with specific information about the person in the video, allowing for accurate aggregation of prediction results.





